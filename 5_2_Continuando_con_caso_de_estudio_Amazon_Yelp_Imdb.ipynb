{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BfDSo7tynVss"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef2I6wwYvU6X",
        "outputId": "9df15b8d-2bcc-4a39-e43f-aa6b0288f5fc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dfa = pd.read_csv('amazon5.txt', sep='\\t', names=['review','label'], header=None, encoding='utf-8')\n",
        "dfi = pd.read_csv('imdb5.txt', delimiter='\\s{3,4}', names=['review','label'], header=None, encoding='utf-8')\n",
        "dfy = pd.read_csv('yelp5.txt', sep='\\t', names=['review','label'], header=None, encoding='utf-8')\n",
        "\n",
        "print('Total de registros de Amazon:', dfa.shape)\n",
        "print('Total de registros de IMBD:', dfi.shape)\n",
        "print('Total de registros de Yelp:', dfy.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxrcJcebns1q",
        "outputId": "4ac8260d-b32c-4cad-f0e4-1e8df5451600"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de registros de Amazon: (1000, 2)\n",
            "Total de registros de IMBD: (1000, 2)\n",
            "Total de registros de Yelp: (1000, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-47a3d1028d09>:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  dfi = pd.read_csv('imdb5.txt', delimiter='\\s{3,4}', names=['review','label'], header=None, encoding='utf-8')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.concat([dfa, dfi, dfy], ignore_index=True)   \n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-vYD-BrodIW",
        "outputId": "c1c573a4-68f2-4db4-da2f-7a8795be7c56"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3000 entries, 0 to 2999\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   review  3000 non-null   object\n",
            " 1   label   3000 non-null   int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 47.0+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['label'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKZoqOrTn5qb",
        "outputId": "a3491e2c-ec8d-4859-8841-4b55b7844cf9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    1500\n",
              "1    1500\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.review     # Serie de strings\n",
        "Y = df.label      # Serie de enteros 0s y 1s\n",
        "\n",
        "assert X.shape == (3000,)         \n",
        "assert Y.shape == (3000,)"
      ],
      "metadata": {
        "id": "WftbOn4MouS4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "negwords = [ 'no', 'nor', 'not', 'ain', 'aren', \"aren't\", 'don', \"don't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
        "mystopwords = [word for word in stopwords.words('english').copy() if word not in negwords]\n",
        "print(len(mystopwords))\n",
        "print(mystopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gHQldvByTGC",
        "outputId": "490f982d-75d0-42da-f2b7-98fd2d53590b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "139\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ma']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_token(doc):\n",
        "  doc = re.sub(r'\\.(?=[^\\s])', '. ', doc)\n",
        "  tokens = word_tokenize(doc)\n",
        "  tokens = [word.lower() for word in tokens]\n",
        "  tokens = [word for word in tokens if word.isalpha()]\n",
        "  tokens = [word for word in tokens if word not in mystopwords]\n",
        "  tokens = [word for word in tokens if len(word) > 1]\n",
        "\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "0BGQpF-MxuWc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEUJxsMSyzPk",
        "outputId": "85867308-a95d-4ad6-9b71-eee4ce584d28"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    So there is no way for me to plug it in here i...\n",
              "1                          Good case, Excellent value.\n",
              "2                               Great for the jawbone.\n",
              "3    Tied to charger for conversations lasting more...\n",
              "4                                    The mic is great.\n",
              "Name: review, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Xcleantok = [clean_token(x) for x in X]   "
      ],
      "metadata": {
        "id": "sSW4UBSoyjau"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x in Xcleantok[0:5]:\n",
        "  print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ay-VSu0Dym9s",
        "outputId": "1b1f5def-5b1a-4294-aa37-11234ac746ec"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['no', 'way', 'plug', 'us', 'unless', 'go', 'converter']\n",
            "['good', 'case', 'excellent', 'value']\n",
            "['great', 'jawbone']\n",
            "['tied', 'charger', 'conversations', 'lasting', 'minutes', 'major', 'problems']\n",
            "['mic', 'great']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_doc(doc):  \n",
        "  tokens = [lemmatizer.lemmatize(word) for word in doc]\n",
        "  # reemplaza cualquier carácter que se repita más de dos veces seguidas por dos ocurrencias de ese carácter. Por ejemplo, 'sooooo good' se convierte en 'soo good'.\n",
        "  tokens = [re.sub(r'(.)\\1{2,}', r'\\1\\1', word) for word in tokens]\n",
        "\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "cHl_LbrGzE22"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xclean = [clean_doc(x) for x in Xcleantok]  \n"
      ],
      "metadata": {
        "id": "4S9BYrOKzUQM"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xclean[0:5]  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRlj0c1nzVFt",
        "outputId": "b01d7359-df8c-40df-aeed-c0317425e2da"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['no', 'way', 'plug', 'u', 'unless', 'go', 'converter'],\n",
              " ['good', 'case', 'excellent', 'value'],\n",
              " ['great', 'jawbone'],\n",
              " ['tied', 'charger', 'conversation', 'lasting', 'minute', 'major', 'problem'],\n",
              " ['mic', 'great']]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_val_and_test, y_train, y_val_and_test = train_test_split(Xclean, Y, train_size=.70, shuffle=True, random_state=1) \n",
        "x_val, x_test, y_val, y_test = train_test_split(x_val_and_test, y_val_and_test, test_size=.50, shuffle=True, random_state=17)\n",
        "\n",
        "print('X,y Train:', len(x_train), len(y_train))      # los \"x_\" son \"list\" y los \"y_\" son \"Series\"\n",
        "print('X,y Val:', len(x_val), len(y_val))\n",
        "print('X,y Test', len(x_test), len(y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAh2iDgezlwb",
        "outputId": "fe026abe-9b71-4840-f9c5-a4a46f130ffc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X,y Train: 2100 2100\n",
            "X,y Val: 450 450\n",
            "X,y Test 450 450\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "midiccionario = Counter()    \n",
        "\n",
        "for k in range(len(x_train)):\n",
        "  midiccionario.update(x_train[k])\n",
        "\n",
        "print('Longitud del diccionario:', len(midiccionario))  \n",
        "print('\\n(word,frequency):') \n",
        "print(midiccionario.most_common(10)) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDAfy1I90wH3",
        "outputId": "abedc6ce-be29-4143-d368-a2b9ff03f325"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Longitud del diccionario: 3595\n",
            "\n",
            "(word,frequency):\n",
            "[('not', 225), ('good', 161), ('movie', 140), ('great', 138), ('phone', 133), ('film', 125), ('time', 100), ('one', 99), ('like', 88), ('work', 86)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(list(np.arange(len(midiccionario))), list(midiccionario.values()), color='blue')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "q0x1k-211P6q",
        "outputId": "74e596ae-8847-4129-bc60-ac6b93b891e5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "min_freq = 2\n",
        "midicc = { word: count for word, count in midiccionario.items() if count >= min_freq}\n",
        "\n",
        "print('Nueva longitud del nuevo vocabulario:', len(midicc))\n",
        "print(list(midicc.items())[0:5])     "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbCB_o461XUm",
        "outputId": "e9e6930e-0cfd-4e5b-ef5c-9950e97a7e86"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nueva longitud del nuevo vocabulario: 1510\n",
            "[('fare', 2), ('much', 39), ('better', 38), ('people', 23), ('like', 88)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_dic = sorted(midiccionario.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "midicc = dict(sorted_dic[:1500])\n",
        "\n",
        "print('Nueva longitud del nuevo vocabulario:', len(midicc))\n",
        "print(list(midicc.items())[0:5]) \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6_gnqH01_94",
        "outputId": "151c3709-1c8a-46c0-ba50-0450fca9d447"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nueva longitud del nuevo vocabulario: 1500\n",
            "[('not', 225), ('good', 161), ('movie', 140), ('great', 138), ('phone', 133)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(list(np.arange(len(midicc))), list(midicc.values()), color='blue')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "_HYoG8RG2PjY",
        "outputId": "4e3d97dc-277d-4f87-f20f-b8b6032de337"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_x = []\n",
        "for ss in x_train:\n",
        "  train_x.append([w for w in ss if w in midicc])\n",
        "\n",
        "val_x = []\n",
        "for ss in x_val:\n",
        "  val_x.append([w for w in ss if w in midicc])\n",
        "\n",
        "test_x = []\n",
        "for ss in x_test:\n",
        "  test_x.append([w for w in ss if w in midicc])"
      ],
      "metadata": {
        "id": "NOKSAiPA84zP"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k in range(3):\n",
        "  print('Antes:', x_train[k])\n",
        "  print('Después:', train_x[k])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37YKODkc86jU",
        "outputId": "6ab07ea9-58a7-457b-a6d3-ea2130e84cd3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Antes: ['fare', 'much', 'better', 'people', 'like', 'morgan', 'freeman', 'jonah', 'hill', 'ed', 'helm', 'wasted']\n",
            "Después: ['fare', 'much', 'better', 'people', 'like', 'morgan', 'ed', 'wasted']\n",
            "Antes: ['tonight', 'elk', 'filet', 'special', 'sucked']\n",
            "Después: ['tonight', 'filet', 'special', 'sucked']\n",
            "Antes: ['paid', 'bill', 'not', 'tip', 'felt', 'server', 'terrible', 'job']\n",
            "Después: ['paid', 'bill', 'not', 'tip', 'felt', 'server', 'terrible', 'job']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#FastText, word2vec de Google y Glove de Stanford\n",
        "\n",
        "| Modelo | Pros | Contras |\n",
        "|---|---|---|\n",
        "|**FastText** (Facebook)|<ul><li>Capaz de generar vectores para palabras fuera del vocabulario entrenado, debido a su enfoque en los n-gramas de caracteres.</li><li>Maneja bien las palabras mal escritas o las palabras con errores de ortografía.</li><li>Soporta más de 150 idiomas.</li><li>Modelos preentrenados fácilmente disponibles.</li></ul>|<ul><li>El tamaño del modelo puede ser bastante grande debido al enfoque en los n-gramas.</li><li>Los vectores generados para palabras fuera del vocabulario pueden no ser siempre semánticamente precisos.</li></ul>|\n",
        "|**Word2Vec** (Google)|<ul><li>Fácil de usar y comprende dos arquitecturas (CBOW y Skip-Gram), lo que ofrece flexibilidad.</li><li>Los vectores de palabras capturan semánticamente las relaciones entre las palabras (por ejemplo, \"rey\" - \"hombre\" + \"mujer\" se acerca a \"reina\").</li><li>Los modelos preentrenados están fácilmente disponibles.</li></ul>|<ul><li>No puede generar vectores para palabras fuera del vocabulario entrenado.</li><li>El rendimiento puede ser menor para palabras raras o mal escritas.</li></ul>|\n",
        "|**GloVe** (Stanford)|<ul><li>Captura tanto la semántica local como la global de las palabras, lo que puede dar lugar a vectores de palabras más precisos.</li><li>Los vectores de palabras capturan semánticamente las relaciones entre las palabras.</li><li>Los modelos preentrenados están fácilmente disponibles.</li></ul>|<ul><li>No puede generar vectores para palabras fuera del vocabulario entrenado.</li><li>El rendimiento puede ser inferior para palabras raras o mal escritas.</li></ul>|\n",
        "\n"
      ],
      "metadata": {
        "id": "E7msOflg-g90"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **FastText**"
      ],
      "metadata": {
        "id": "-gZFPkLY92V3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-seQNxqI-PhR",
        "outputId": "855ec6f2-80ed-4872-bd6d-19a91660c57c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.10/dist-packages (0.9.2)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.10/dist-packages (from fasttext) (2.10.4)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.22.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import fasttext\n",
        "import fasttext.util\n",
        "fasttext.util.download_model('en', if_exists='ignore')  # English\n",
        "ft = fasttext.load_model('cc.en.300.bin')\n",
        "# Construimos el diccionario de palabras y sus vectores embebidos\n",
        "embedding_dict = {word: ft.get_word_vector(word) for word in midicc.keys()}\n",
        "\n",
        "# Guardamos el diccionario para uso futuro\n",
        "with open('embedding_dict.pickle', 'wb') as handle:\n",
        "    pickle.dump(embedding_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# Borramos la variable de FastText para liberar memoria RAM\n",
        "del ft"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPLjbye03Go_",
        "outputId": "a76f68c0-0dde-4af6-e0d1-72bea1bb0293"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def comment_to_vector(comment):\n",
        "    embeddings = []\n",
        "    for word in comment:\n",
        "        embedding = embedding_dict.get(word, np.zeros(300))\n",
        "        embeddings.append(embedding)\n",
        "    if embeddings:\n",
        "        average_embedding = np.mean(embeddings, axis=0)\n",
        "    else:\n",
        "        average_embedding = np.zeros(300)  \n",
        "    return average_embedding\n"
      ],
      "metadata": {
        "id": "0kcmrh1g_r9C"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainEmb = np.array([comment_to_vector(comment) for comment in train_x])\n",
        "\n",
        "valEmb = np.array([comment_to_vector(comment) for comment in val_x])\n",
        "\n",
        "testEmb = np.array([comment_to_vector(comment) for comment in test_x])"
      ],
      "metadata": {
        "id": "Gz1wvRmw-do7"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB"
      ],
      "metadata": {
        "id": "FGB9DMI9AiTP"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modeloLR = LogisticRegression(max_iter=1000)\n",
        "modeloLR.fit(trainEmb, y_train)\n",
        "\n",
        "modeloRF = RandomForestClassifier(n_estimators=100, random_state=0)\n",
        "modeloRF.fit(trainEmb, y_train)\n",
        "\n",
        "\n",
        "print('LR: Train-accuracy: %.2f%%' % (100*modeloLR.score(trainEmb, y_train)))\n",
        "print('LR: Val-accuracy: %2.f%%' % (100*modeloLR.score(valEmb, y_val)))\n",
        "\n",
        "print('\\nRF: Train-accuracy: %.2f%%' % (100*modeloRF.score(trainEmb, y_train)))\n",
        "print('RF: Val-accuracy: %.2f%%' % (100*modeloRF.score(valEmb, y_val)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jq6vY6RdAkaj",
        "outputId": "1ba98253-9480-43e1-e263-f79fe845f3b9"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LR: Train-accuracy: 84.43%\n",
            "LR: Val-accuracy: 82%\n",
            "\n",
            "RF: Train-accuracy: 99.62%\n",
            "RF: Val-accuracy: 80.22%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "print('Test-accuracy con el mejor modelo Tf-idf %.2f%%' % (100*modeloLR.score(testEmb, y_test)))\n",
        "\n",
        "pred = modeloLR.predict(testEmb)\n",
        "print('\\nMatriz de confusión con el mejor modelo Tf-idf:')\n",
        "print(confusion_matrix(y_test, pred, labels=[0,1]))\n",
        "\n",
        "print('\\nMatriz de confusión con el mejor modelo de Tf-idf en proporciones:')\n",
        "print(confusion_matrix(y_test, pred, labels=[0,1]) / pred.shape[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3cz10vJBoSe",
        "outputId": "3c9a8227-c4f3-433e-f748-2049434059a6"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test-accuracy con el mejor modelo Tf-idf 82.22%\n",
            "\n",
            "Matriz de confusión con el mejor modelo Tf-idf:\n",
            "[[175  41]\n",
            " [ 39 195]]\n",
            "\n",
            "Matriz de confusión con el mejor modelo de Tf-idf en proporciones:\n",
            "[[0.38888889 0.09111111]\n",
            " [0.08666667 0.43333333]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100], \n",
        "    'penalty': ['l1', 'l2'], \n",
        "    'solver': ['liblinear', 'saga']\n",
        "} \n",
        "\n",
        "grid = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=5, verbose=3)\n",
        "grid.fit(trainEmb, y_train)\n",
        "\n",
        "print(\"Best parameters found: \", grid.best_params_)\n",
        "print(\"Best score found: \", grid.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_1roAsII5CB",
        "outputId": "30cee82b-3107-465a-f584-b33075fb4b91"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
            "[CV 1/5] END C=0.1, penalty=l1, solver=liblinear;, score=0.555 total time=   0.1s\n",
            "[CV 2/5] END C=0.1, penalty=l1, solver=liblinear;, score=0.610 total time=   0.1s\n",
            "[CV 3/5] END C=0.1, penalty=l1, solver=liblinear;, score=0.567 total time=   0.1s\n",
            "[CV 4/5] END C=0.1, penalty=l1, solver=liblinear;, score=0.507 total time=   0.1s\n",
            "[CV 5/5] END C=0.1, penalty=l1, solver=liblinear;, score=0.588 total time=   0.1s\n",
            "[CV 1/5] END ....C=0.1, penalty=l1, solver=saga;, score=0.557 total time=   1.3s\n",
            "[CV 2/5] END ....C=0.1, penalty=l1, solver=saga;, score=0.617 total time=   1.3s\n",
            "[CV 3/5] END ....C=0.1, penalty=l1, solver=saga;, score=0.550 total time=   1.2s\n",
            "[CV 4/5] END ....C=0.1, penalty=l1, solver=saga;, score=0.507 total time=   0.2s\n",
            "[CV 5/5] END ....C=0.1, penalty=l1, solver=saga;, score=0.507 total time=   0.3s\n",
            "[CV 1/5] END C=0.1, penalty=l2, solver=liblinear;, score=0.736 total time=   0.0s\n",
            "[CV 2/5] END C=0.1, penalty=l2, solver=liblinear;, score=0.798 total time=   0.0s\n",
            "[CV 3/5] END C=0.1, penalty=l2, solver=liblinear;, score=0.819 total time=   0.0s\n",
            "[CV 4/5] END C=0.1, penalty=l2, solver=liblinear;, score=0.805 total time=   0.0s\n",
            "[CV 5/5] END C=0.1, penalty=l2, solver=liblinear;, score=0.767 total time=   0.1s\n",
            "[CV 1/5] END ....C=0.1, penalty=l2, solver=saga;, score=0.743 total time=   0.2s\n",
            "[CV 2/5] END ....C=0.1, penalty=l2, solver=saga;, score=0.793 total time=   0.2s\n",
            "[CV 3/5] END ....C=0.1, penalty=l2, solver=saga;, score=0.819 total time=   0.2s\n",
            "[CV 4/5] END ....C=0.1, penalty=l2, solver=saga;, score=0.800 total time=   0.2s\n",
            "[CV 5/5] END ....C=0.1, penalty=l2, solver=saga;, score=0.767 total time=   0.2s\n",
            "[CV 1/5] END .C=1, penalty=l1, solver=liblinear;, score=0.786 total time=   0.1s\n",
            "[CV 2/5] END .C=1, penalty=l1, solver=liblinear;, score=0.805 total time=   0.1s\n",
            "[CV 3/5] END .C=1, penalty=l1, solver=liblinear;, score=0.833 total time=   0.1s\n",
            "[CV 4/5] END .C=1, penalty=l1, solver=liblinear;, score=0.824 total time=   0.1s\n",
            "[CV 5/5] END .C=1, penalty=l1, solver=liblinear;, score=0.771 total time=   0.1s\n",
            "[CV 1/5] END ......C=1, penalty=l1, solver=saga;, score=0.779 total time=   4.3s\n",
            "[CV 2/5] END ......C=1, penalty=l1, solver=saga;, score=0.800 total time=   4.7s\n",
            "[CV 3/5] END ......C=1, penalty=l1, solver=saga;, score=0.833 total time=   4.0s\n",
            "[CV 4/5] END ......C=1, penalty=l1, solver=saga;, score=0.824 total time=   4.6s\n",
            "[CV 5/5] END ......C=1, penalty=l1, solver=saga;, score=0.771 total time=   2.2s\n",
            "[CV 1/5] END .C=1, penalty=l2, solver=liblinear;, score=0.795 total time=   0.1s\n",
            "[CV 2/5] END .C=1, penalty=l2, solver=liblinear;, score=0.824 total time=   0.1s\n",
            "[CV 3/5] END .C=1, penalty=l2, solver=liblinear;, score=0.836 total time=   0.1s\n",
            "[CV 4/5] END .C=1, penalty=l2, solver=liblinear;, score=0.819 total time=   0.1s\n",
            "[CV 5/5] END .C=1, penalty=l2, solver=liblinear;, score=0.795 total time=   0.2s\n",
            "[CV 1/5] END ......C=1, penalty=l2, solver=saga;, score=0.795 total time=   0.9s\n",
            "[CV 2/5] END ......C=1, penalty=l2, solver=saga;, score=0.824 total time=   0.6s\n",
            "[CV 3/5] END ......C=1, penalty=l2, solver=saga;, score=0.836 total time=   0.5s\n",
            "[CV 4/5] END ......C=1, penalty=l2, solver=saga;, score=0.819 total time=   0.5s\n",
            "[CV 5/5] END ......C=1, penalty=l2, solver=saga;, score=0.795 total time=   0.3s\n",
            "[CV 1/5] END C=10, penalty=l1, solver=liblinear;, score=0.790 total time=   0.4s\n",
            "[CV 2/5] END C=10, penalty=l1, solver=liblinear;, score=0.821 total time=   0.8s\n",
            "[CV 3/5] END C=10, penalty=l1, solver=liblinear;, score=0.838 total time=   0.9s\n",
            "[CV 4/5] END C=10, penalty=l1, solver=liblinear;, score=0.850 total time=   0.9s\n",
            "[CV 5/5] END C=10, penalty=l1, solver=liblinear;, score=0.790 total time=   1.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5] END .....C=10, penalty=l1, solver=saga;, score=0.790 total time=   9.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5] END .....C=10, penalty=l1, solver=saga;, score=0.814 total time=   9.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5] END .....C=10, penalty=l1, solver=saga;, score=0.836 total time=   8.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5] END .....C=10, penalty=l1, solver=saga;, score=0.843 total time=   9.8s\n",
            "[CV 5/5] END .....C=10, penalty=l1, solver=saga;, score=0.788 total time=   7.5s\n",
            "[CV 1/5] END C=10, penalty=l2, solver=liblinear;, score=0.805 total time=   0.1s\n",
            "[CV 2/5] END C=10, penalty=l2, solver=liblinear;, score=0.829 total time=   0.1s\n",
            "[CV 3/5] END C=10, penalty=l2, solver=liblinear;, score=0.850 total time=   0.1s\n",
            "[CV 4/5] END C=10, penalty=l2, solver=liblinear;, score=0.836 total time=   0.1s\n",
            "[CV 5/5] END C=10, penalty=l2, solver=liblinear;, score=0.788 total time=   0.1s\n",
            "[CV 1/5] END .....C=10, penalty=l2, solver=saga;, score=0.805 total time=   2.6s\n",
            "[CV 2/5] END .....C=10, penalty=l2, solver=saga;, score=0.831 total time=   2.4s\n",
            "[CV 3/5] END .....C=10, penalty=l2, solver=saga;, score=0.850 total time=   2.4s\n",
            "[CV 4/5] END .....C=10, penalty=l2, solver=saga;, score=0.833 total time=   3.0s\n",
            "[CV 5/5] END .....C=10, penalty=l2, solver=saga;, score=0.788 total time=   1.6s\n",
            "[CV 1/5] END C=100, penalty=l1, solver=liblinear;, score=0.769 total time=   4.1s\n",
            "[CV 2/5] END C=100, penalty=l1, solver=liblinear;, score=0.810 total time=   5.1s\n",
            "[CV 3/5] END C=100, penalty=l1, solver=liblinear;, score=0.812 total time=   4.6s\n",
            "[CV 4/5] END C=100, penalty=l1, solver=liblinear;, score=0.821 total time=   4.4s\n",
            "[CV 5/5] END C=100, penalty=l1, solver=liblinear;, score=0.793 total time=   2.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5] END ....C=100, penalty=l1, solver=saga;, score=0.790 total time=  10.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5] END ....C=100, penalty=l1, solver=saga;, score=0.812 total time=  10.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5] END ....C=100, penalty=l1, solver=saga;, score=0.829 total time=   9.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5] END ....C=100, penalty=l1, solver=saga;, score=0.826 total time=  10.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 5/5] END ....C=100, penalty=l1, solver=saga;, score=0.788 total time=  10.0s\n",
            "[CV 1/5] END C=100, penalty=l2, solver=liblinear;, score=0.793 total time=   0.2s\n",
            "[CV 2/5] END C=100, penalty=l2, solver=liblinear;, score=0.821 total time=   0.2s\n",
            "[CV 3/5] END C=100, penalty=l2, solver=liblinear;, score=0.836 total time=   0.2s\n",
            "[CV 4/5] END C=100, penalty=l2, solver=liblinear;, score=0.831 total time=   0.3s\n",
            "[CV 5/5] END C=100, penalty=l2, solver=liblinear;, score=0.793 total time=   0.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 1/5] END ....C=100, penalty=l2, solver=saga;, score=0.800 total time=   8.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 2/5] END ....C=100, penalty=l2, solver=saga;, score=0.824 total time=   7.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 3/5] END ....C=100, penalty=l2, solver=saga;, score=0.838 total time=   8.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV 4/5] END ....C=100, penalty=l2, solver=saga;, score=0.836 total time=   8.2s\n",
            "[CV 5/5] END ....C=100, penalty=l2, solver=saga;, score=0.793 total time=   4.6s\n",
            "Best parameters found:  {'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}\n",
            "Best score found:  0.8214285714285715\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = grid.predict(valEmb)\n",
        "print('LR (with Grid Search): Val-accuracy: %.2f%%' % (100*accuracy_score(y_val, y_pred)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3VZidjWKH5v",
        "outputId": "79294e6c-42ec-4068-bec2-af95d9a93ea5"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LR (with Grid Search): Val-accuracy: 84.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "print('Test-accuracy con el mejor modelo Tf-idf %.2f%%' % (100*grid.score(testEmb, y_test)))\n",
        "\n",
        "pred = grid.predict(testEmb)\n",
        "print('\\nMatriz de confusión con el mejor modelo Tf-idf:')\n",
        "print(confusion_matrix(y_test, pred, labels=[0,1]))\n",
        "\n",
        "print('\\nMatriz de confusión con el mejor modelo de Tf-idf en proporciones:')\n",
        "print(confusion_matrix(y_test, pred, labels=[0,1]) / pred.shape[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjB-ZbUwKOau",
        "outputId": "2eb92df8-c3d5-495e-db3a-2c83e82ecc9e"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test-accuracy con el mejor modelo Tf-idf 82.22%\n",
            "\n",
            "Matriz de confusión con el mejor modelo Tf-idf:\n",
            "[[178  38]\n",
            " [ 42 192]]\n",
            "\n",
            "Matriz de confusión con el mejor modelo de Tf-idf en proporciones:\n",
            "[[0.39555556 0.08444444]\n",
            " [0.09333333 0.42666667]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lightgbm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZWUL6ZfCO6i",
        "outputId": "331bb01e-0eb9-4aa8-8ef5-c40ae46cb1f1"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.10/dist-packages (3.3.5)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from lightgbm) (0.40.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lightgbm) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lightgbm) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn!=0.22.0 in /usr/local/lib/python3.10/dist-packages (from lightgbm) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn!=0.22.0->lightgbm) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn!=0.22.0->lightgbm) (3.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "modeloLGBM = LGBMClassifier()\n",
        "modeloLGBM.fit(trainEmb, y_train)\n",
        "print('LGBM: Train-accuracy: %.2f%%' % (100*modeloLGBM.score(trainEmb, y_train)))\n",
        "print('LGBM: Val-accuracy: %.2f%%' % (100*modeloLGBM.score(valEmb, y_val)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TUsQUpECt6k",
        "outputId": "eccfef44-c840-41b3-9cd7-6534dab78379"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LGBM: Train-accuracy: 99.62%\n",
            "LGBM: Val-accuracy: 80.67%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'learning_rate': [0.01, 0.1, 1],\n",
        "    'max_depth': [5, 10, 15]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(LGBMClassifier(), param_grid, cv=5, verbose=10)\n",
        "grid.fit(trainEmb, y_train)\n",
        "\n",
        "print(\"Best parameters found: \", grid.best_params_)\n",
        "print(\"Best score found: \", grid.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cyRES9EC2SJ",
        "outputId": "0d52abe5-462e-45d8-d92f-dada211859d1"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
            "[CV 1/5; 1/27] START learning_rate=0.01, max_depth=5, n_estimators=100..........\n",
            "[CV 1/5; 1/27] END learning_rate=0.01, max_depth=5, n_estimators=100;, score=0.740 total time=   4.6s\n",
            "[CV 2/5; 1/27] START learning_rate=0.01, max_depth=5, n_estimators=100..........\n",
            "[CV 2/5; 1/27] END learning_rate=0.01, max_depth=5, n_estimators=100;, score=0.738 total time=   2.1s\n",
            "[CV 3/5; 1/27] START learning_rate=0.01, max_depth=5, n_estimators=100..........\n",
            "[CV 3/5; 1/27] END learning_rate=0.01, max_depth=5, n_estimators=100;, score=0.774 total time=   5.7s\n",
            "[CV 4/5; 1/27] START learning_rate=0.01, max_depth=5, n_estimators=100..........\n",
            "[CV 4/5; 1/27] END learning_rate=0.01, max_depth=5, n_estimators=100;, score=0.795 total time=   2.2s\n",
            "[CV 5/5; 1/27] START learning_rate=0.01, max_depth=5, n_estimators=100..........\n",
            "[CV 5/5; 1/27] END learning_rate=0.01, max_depth=5, n_estimators=100;, score=0.762 total time=   2.1s\n",
            "[CV 1/5; 2/27] START learning_rate=0.01, max_depth=5, n_estimators=200..........\n",
            "[CV 1/5; 2/27] END learning_rate=0.01, max_depth=5, n_estimators=200;, score=0.736 total time=   4.1s\n",
            "[CV 2/5; 2/27] START learning_rate=0.01, max_depth=5, n_estimators=200..........\n",
            "[CV 2/5; 2/27] END learning_rate=0.01, max_depth=5, n_estimators=200;, score=0.779 total time=   7.1s\n",
            "[CV 3/5; 2/27] START learning_rate=0.01, max_depth=5, n_estimators=200..........\n",
            "[CV 3/5; 2/27] END learning_rate=0.01, max_depth=5, n_estimators=200;, score=0.793 total time=   4.1s\n",
            "[CV 4/5; 2/27] START learning_rate=0.01, max_depth=5, n_estimators=200..........\n",
            "[CV 4/5; 2/27] END learning_rate=0.01, max_depth=5, n_estimators=200;, score=0.805 total time=   6.1s\n",
            "[CV 5/5; 2/27] START learning_rate=0.01, max_depth=5, n_estimators=200..........\n",
            "[CV 5/5; 2/27] END learning_rate=0.01, max_depth=5, n_estimators=200;, score=0.779 total time=   4.1s\n",
            "[CV 1/5; 3/27] START learning_rate=0.01, max_depth=5, n_estimators=300..........\n",
            "[CV 1/5; 3/27] END learning_rate=0.01, max_depth=5, n_estimators=300;, score=0.762 total time=   8.0s\n",
            "[CV 2/5; 3/27] START learning_rate=0.01, max_depth=5, n_estimators=300..........\n",
            "[CV 2/5; 3/27] END learning_rate=0.01, max_depth=5, n_estimators=300;, score=0.788 total time=  19.7s\n",
            "[CV 3/5; 3/27] START learning_rate=0.01, max_depth=5, n_estimators=300..........\n",
            "[CV 3/5; 3/27] END learning_rate=0.01, max_depth=5, n_estimators=300;, score=0.802 total time=   9.5s\n",
            "[CV 4/5; 3/27] START learning_rate=0.01, max_depth=5, n_estimators=300..........\n",
            "[CV 4/5; 3/27] END learning_rate=0.01, max_depth=5, n_estimators=300;, score=0.838 total time=   8.1s\n",
            "[CV 5/5; 3/27] START learning_rate=0.01, max_depth=5, n_estimators=300..........\n",
            "[CV 5/5; 3/27] END learning_rate=0.01, max_depth=5, n_estimators=300;, score=0.783 total time=   6.0s\n",
            "[CV 1/5; 4/27] START learning_rate=0.01, max_depth=10, n_estimators=100.........\n",
            "[CV 1/5; 4/27] END learning_rate=0.01, max_depth=10, n_estimators=100;, score=0.740 total time=   5.3s\n",
            "[CV 2/5; 4/27] START learning_rate=0.01, max_depth=10, n_estimators=100.........\n",
            "[CV 2/5; 4/27] END learning_rate=0.01, max_depth=10, n_estimators=100;, score=0.733 total time=   4.1s\n",
            "[CV 3/5; 4/27] START learning_rate=0.01, max_depth=10, n_estimators=100.........\n",
            "[CV 3/5; 4/27] END learning_rate=0.01, max_depth=10, n_estimators=100;, score=0.769 total time=   3.4s\n",
            "[CV 4/5; 4/27] START learning_rate=0.01, max_depth=10, n_estimators=100.........\n",
            "[CV 4/5; 4/27] END learning_rate=0.01, max_depth=10, n_estimators=100;, score=0.783 total time=   5.2s\n",
            "[CV 5/5; 4/27] START learning_rate=0.01, max_depth=10, n_estimators=100.........\n",
            "[CV 5/5; 4/27] END learning_rate=0.01, max_depth=10, n_estimators=100;, score=0.750 total time=   3.7s\n",
            "[CV 1/5; 5/27] START learning_rate=0.01, max_depth=10, n_estimators=200.........\n",
            "[CV 1/5; 5/27] END learning_rate=0.01, max_depth=10, n_estimators=200;, score=0.757 total time=   8.6s\n",
            "[CV 2/5; 5/27] START learning_rate=0.01, max_depth=10, n_estimators=200.........\n",
            "[CV 2/5; 5/27] END learning_rate=0.01, max_depth=10, n_estimators=200;, score=0.764 total time=   6.4s\n",
            "[CV 3/5; 5/27] START learning_rate=0.01, max_depth=10, n_estimators=200.........\n",
            "[CV 3/5; 5/27] END learning_rate=0.01, max_depth=10, n_estimators=200;, score=0.798 total time=   8.5s\n",
            "[CV 4/5; 5/27] START learning_rate=0.01, max_depth=10, n_estimators=200.........\n",
            "[CV 4/5; 5/27] END learning_rate=0.01, max_depth=10, n_estimators=200;, score=0.821 total time=   8.4s\n",
            "[CV 5/5; 5/27] START learning_rate=0.01, max_depth=10, n_estimators=200.........\n",
            "[CV 5/5; 5/27] END learning_rate=0.01, max_depth=10, n_estimators=200;, score=0.774 total time=   6.4s\n",
            "[CV 1/5; 6/27] START learning_rate=0.01, max_depth=10, n_estimators=300.........\n",
            "[CV 1/5; 6/27] END learning_rate=0.01, max_depth=10, n_estimators=300;, score=0.760 total time=  11.7s\n",
            "[CV 2/5; 6/27] START learning_rate=0.01, max_depth=10, n_estimators=300.........\n",
            "[CV 2/5; 6/27] END learning_rate=0.01, max_depth=10, n_estimators=300;, score=0.774 total time=  11.6s\n",
            "[CV 3/5; 6/27] START learning_rate=0.01, max_depth=10, n_estimators=300.........\n",
            "[CV 3/5; 6/27] END learning_rate=0.01, max_depth=10, n_estimators=300;, score=0.824 total time=  11.7s\n",
            "[CV 4/5; 6/27] START learning_rate=0.01, max_depth=10, n_estimators=300.........\n",
            "[CV 4/5; 6/27] END learning_rate=0.01, max_depth=10, n_estimators=300;, score=0.845 total time=  11.7s\n",
            "[CV 5/5; 6/27] START learning_rate=0.01, max_depth=10, n_estimators=300.........\n",
            "[CV 5/5; 6/27] END learning_rate=0.01, max_depth=10, n_estimators=300;, score=0.781 total time=  11.6s\n",
            "[CV 1/5; 7/27] START learning_rate=0.01, max_depth=15, n_estimators=100.........\n",
            "[CV 1/5; 7/27] END learning_rate=0.01, max_depth=15, n_estimators=100;, score=0.740 total time=   3.2s\n",
            "[CV 2/5; 7/27] START learning_rate=0.01, max_depth=15, n_estimators=100.........\n",
            "[CV 2/5; 7/27] END learning_rate=0.01, max_depth=15, n_estimators=100;, score=0.733 total time=   5.2s\n",
            "[CV 3/5; 7/27] START learning_rate=0.01, max_depth=15, n_estimators=100.........\n",
            "[CV 3/5; 7/27] END learning_rate=0.01, max_depth=15, n_estimators=100;, score=0.769 total time=   3.3s\n",
            "[CV 4/5; 7/27] START learning_rate=0.01, max_depth=15, n_estimators=100.........\n",
            "[CV 4/5; 7/27] END learning_rate=0.01, max_depth=15, n_estimators=100;, score=0.779 total time=   3.2s\n",
            "[CV 5/5; 7/27] START learning_rate=0.01, max_depth=15, n_estimators=100.........\n",
            "[CV 5/5; 7/27] END learning_rate=0.01, max_depth=15, n_estimators=100;, score=0.750 total time=   5.2s\n",
            "[CV 1/5; 8/27] START learning_rate=0.01, max_depth=15, n_estimators=200.........\n",
            "[CV 1/5; 8/27] END learning_rate=0.01, max_depth=15, n_estimators=200;, score=0.757 total time=   6.5s\n",
            "[CV 2/5; 8/27] START learning_rate=0.01, max_depth=15, n_estimators=200.........\n",
            "[CV 2/5; 8/27] END learning_rate=0.01, max_depth=15, n_estimators=200;, score=0.764 total time=   8.4s\n",
            "[CV 3/5; 8/27] START learning_rate=0.01, max_depth=15, n_estimators=200.........\n",
            "[CV 3/5; 8/27] END learning_rate=0.01, max_depth=15, n_estimators=200;, score=0.798 total time=   6.5s\n",
            "[CV 4/5; 8/27] START learning_rate=0.01, max_depth=15, n_estimators=200.........\n",
            "[CV 4/5; 8/27] END learning_rate=0.01, max_depth=15, n_estimators=200;, score=0.819 total time=   8.4s\n",
            "[CV 5/5; 8/27] START learning_rate=0.01, max_depth=15, n_estimators=200.........\n",
            "[CV 5/5; 8/27] END learning_rate=0.01, max_depth=15, n_estimators=200;, score=0.769 total time=   8.4s\n",
            "[CV 1/5; 9/27] START learning_rate=0.01, max_depth=15, n_estimators=300.........\n",
            "[CV 1/5; 9/27] END learning_rate=0.01, max_depth=15, n_estimators=300;, score=0.762 total time=  11.7s\n",
            "[CV 2/5; 9/27] START learning_rate=0.01, max_depth=15, n_estimators=300.........\n",
            "[CV 2/5; 9/27] END learning_rate=0.01, max_depth=15, n_estimators=300;, score=0.774 total time=  11.6s\n",
            "[CV 3/5; 9/27] START learning_rate=0.01, max_depth=15, n_estimators=300.........\n",
            "[CV 3/5; 9/27] END learning_rate=0.01, max_depth=15, n_estimators=300;, score=0.812 total time=  11.7s\n",
            "[CV 4/5; 9/27] START learning_rate=0.01, max_depth=15, n_estimators=300.........\n",
            "[CV 4/5; 9/27] END learning_rate=0.01, max_depth=15, n_estimators=300;, score=0.843 total time=  11.7s\n",
            "[CV 5/5; 9/27] START learning_rate=0.01, max_depth=15, n_estimators=300.........\n",
            "[CV 5/5; 9/27] END learning_rate=0.01, max_depth=15, n_estimators=300;, score=0.779 total time=   9.9s\n",
            "[CV 1/5; 10/27] START learning_rate=0.1, max_depth=5, n_estimators=100..........\n",
            "[CV 1/5; 10/27] END learning_rate=0.1, max_depth=5, n_estimators=100;, score=0.776 total time=   3.6s\n",
            "[CV 2/5; 10/27] START learning_rate=0.1, max_depth=5, n_estimators=100..........\n",
            "[CV 2/5; 10/27] END learning_rate=0.1, max_depth=5, n_estimators=100;, score=0.783 total time=   1.9s\n",
            "[CV 3/5; 10/27] START learning_rate=0.1, max_depth=5, n_estimators=100..........\n",
            "[CV 3/5; 10/27] END learning_rate=0.1, max_depth=5, n_estimators=100;, score=0.840 total time=   1.9s\n",
            "[CV 4/5; 10/27] START learning_rate=0.1, max_depth=5, n_estimators=100..........\n",
            "[CV 4/5; 10/27] END learning_rate=0.1, max_depth=5, n_estimators=100;, score=0.829 total time=   2.0s\n",
            "[CV 5/5; 10/27] START learning_rate=0.1, max_depth=5, n_estimators=100..........\n",
            "[CV 5/5; 10/27] END learning_rate=0.1, max_depth=5, n_estimators=100;, score=0.795 total time=   1.9s\n",
            "[CV 1/5; 11/27] START learning_rate=0.1, max_depth=5, n_estimators=200..........\n",
            "[CV 1/5; 11/27] END learning_rate=0.1, max_depth=5, n_estimators=200;, score=0.762 total time=   5.7s\n",
            "[CV 2/5; 11/27] START learning_rate=0.1, max_depth=5, n_estimators=200..........\n",
            "[CV 2/5; 11/27] END learning_rate=0.1, max_depth=5, n_estimators=200;, score=0.788 total time=   3.7s\n",
            "[CV 3/5; 11/27] START learning_rate=0.1, max_depth=5, n_estimators=200..........\n",
            "[CV 3/5; 11/27] END learning_rate=0.1, max_depth=5, n_estimators=200;, score=0.829 total time=   5.7s\n",
            "[CV 4/5; 11/27] START learning_rate=0.1, max_depth=5, n_estimators=200..........\n",
            "[CV 4/5; 11/27] END learning_rate=0.1, max_depth=5, n_estimators=200;, score=0.829 total time=   3.7s\n",
            "[CV 5/5; 11/27] START learning_rate=0.1, max_depth=5, n_estimators=200..........\n",
            "[CV 5/5; 11/27] END learning_rate=0.1, max_depth=5, n_estimators=200;, score=0.800 total time=   3.6s\n",
            "[CV 1/5; 12/27] START learning_rate=0.1, max_depth=5, n_estimators=300..........\n",
            "[CV 1/5; 12/27] END learning_rate=0.1, max_depth=5, n_estimators=300;, score=0.757 total time=   7.3s\n",
            "[CV 2/5; 12/27] START learning_rate=0.1, max_depth=5, n_estimators=300..........\n",
            "[CV 2/5; 12/27] END learning_rate=0.1, max_depth=5, n_estimators=300;, score=0.781 total time=   5.4s\n",
            "[CV 3/5; 12/27] START learning_rate=0.1, max_depth=5, n_estimators=300..........\n",
            "[CV 3/5; 12/27] END learning_rate=0.1, max_depth=5, n_estimators=300;, score=0.829 total time=   7.2s\n",
            "[CV 4/5; 12/27] START learning_rate=0.1, max_depth=5, n_estimators=300..........\n",
            "[CV 4/5; 12/27] END learning_rate=0.1, max_depth=5, n_estimators=300;, score=0.831 total time=   5.3s\n",
            "[CV 5/5; 12/27] START learning_rate=0.1, max_depth=5, n_estimators=300..........\n",
            "[CV 5/5; 12/27] END learning_rate=0.1, max_depth=5, n_estimators=300;, score=0.798 total time=   7.2s\n",
            "[CV 1/5; 13/27] START learning_rate=0.1, max_depth=10, n_estimators=100.........\n",
            "[CV 1/5; 13/27] END learning_rate=0.1, max_depth=10, n_estimators=100;, score=0.769 total time=   3.4s\n",
            "[CV 2/5; 13/27] START learning_rate=0.1, max_depth=10, n_estimators=100.........\n",
            "[CV 2/5; 13/27] END learning_rate=0.1, max_depth=10, n_estimators=100;, score=0.793 total time=   5.4s\n",
            "[CV 3/5; 13/27] START learning_rate=0.1, max_depth=10, n_estimators=100.........\n",
            "[CV 3/5; 13/27] END learning_rate=0.1, max_depth=10, n_estimators=100;, score=0.843 total time=   3.4s\n",
            "[CV 4/5; 13/27] START learning_rate=0.1, max_depth=10, n_estimators=100.........\n",
            "[CV 4/5; 13/27] END learning_rate=0.1, max_depth=10, n_estimators=100;, score=0.836 total time=   3.3s\n",
            "[CV 5/5; 13/27] START learning_rate=0.1, max_depth=10, n_estimators=100.........\n",
            "[CV 5/5; 13/27] END learning_rate=0.1, max_depth=10, n_estimators=100;, score=0.793 total time=   5.4s\n",
            "[CV 1/5; 14/27] START learning_rate=0.1, max_depth=10, n_estimators=200.........\n",
            "[CV 1/5; 14/27] END learning_rate=0.1, max_depth=10, n_estimators=200;, score=0.767 total time=   6.6s\n",
            "[CV 2/5; 14/27] START learning_rate=0.1, max_depth=10, n_estimators=200.........\n",
            "[CV 2/5; 14/27] END learning_rate=0.1, max_depth=10, n_estimators=200;, score=0.790 total time=   8.6s\n",
            "[CV 3/5; 14/27] START learning_rate=0.1, max_depth=10, n_estimators=200.........\n",
            "[CV 3/5; 14/27] END learning_rate=0.1, max_depth=10, n_estimators=200;, score=0.845 total time=   6.5s\n",
            "[CV 4/5; 14/27] START learning_rate=0.1, max_depth=10, n_estimators=200.........\n",
            "[CV 4/5; 14/27] END learning_rate=0.1, max_depth=10, n_estimators=200;, score=0.836 total time=   8.5s\n",
            "[CV 5/5; 14/27] START learning_rate=0.1, max_depth=10, n_estimators=200.........\n",
            "[CV 5/5; 14/27] END learning_rate=0.1, max_depth=10, n_estimators=200;, score=0.798 total time=   8.4s\n",
            "[CV 1/5; 15/27] START learning_rate=0.1, max_depth=10, n_estimators=300.........\n",
            "[CV 1/5; 15/27] END learning_rate=0.1, max_depth=10, n_estimators=300;, score=0.774 total time=  11.3s\n",
            "[CV 2/5; 15/27] START learning_rate=0.1, max_depth=10, n_estimators=300.........\n",
            "[CV 2/5; 15/27] END learning_rate=0.1, max_depth=10, n_estimators=300;, score=0.788 total time=  11.3s\n",
            "[CV 3/5; 15/27] START learning_rate=0.1, max_depth=10, n_estimators=300.........\n",
            "[CV 3/5; 15/27] END learning_rate=0.1, max_depth=10, n_estimators=300;, score=0.848 total time=  11.4s\n",
            "[CV 4/5; 15/27] START learning_rate=0.1, max_depth=10, n_estimators=300.........\n",
            "[CV 4/5; 15/27] END learning_rate=0.1, max_depth=10, n_estimators=300;, score=0.838 total time=   9.5s\n",
            "[CV 5/5; 15/27] START learning_rate=0.1, max_depth=10, n_estimators=300.........\n",
            "[CV 5/5; 15/27] END learning_rate=0.1, max_depth=10, n_estimators=300;, score=0.802 total time=  11.3s\n",
            "[CV 1/5; 16/27] START learning_rate=0.1, max_depth=15, n_estimators=100.........\n",
            "[CV 1/5; 16/27] END learning_rate=0.1, max_depth=15, n_estimators=100;, score=0.764 total time=   5.4s\n",
            "[CV 2/5; 16/27] START learning_rate=0.1, max_depth=15, n_estimators=100.........\n",
            "[CV 2/5; 16/27] END learning_rate=0.1, max_depth=15, n_estimators=100;, score=0.800 total time=   3.4s\n",
            "[CV 3/5; 16/27] START learning_rate=0.1, max_depth=15, n_estimators=100.........\n",
            "[CV 3/5; 16/27] END learning_rate=0.1, max_depth=15, n_estimators=100;, score=0.838 total time=   3.5s\n",
            "[CV 4/5; 16/27] START learning_rate=0.1, max_depth=15, n_estimators=100.........\n",
            "[CV 4/5; 16/27] END learning_rate=0.1, max_depth=15, n_estimators=100;, score=0.826 total time=   5.4s\n",
            "[CV 5/5; 16/27] START learning_rate=0.1, max_depth=15, n_estimators=100.........\n",
            "[CV 5/5; 16/27] END learning_rate=0.1, max_depth=15, n_estimators=100;, score=0.783 total time=   3.4s\n",
            "[CV 1/5; 17/27] START learning_rate=0.1, max_depth=15, n_estimators=200.........\n",
            "[CV 1/5; 17/27] END learning_rate=0.1, max_depth=15, n_estimators=200;, score=0.771 total time=   8.7s\n",
            "[CV 2/5; 17/27] START learning_rate=0.1, max_depth=15, n_estimators=200.........\n",
            "[CV 2/5; 17/27] END learning_rate=0.1, max_depth=15, n_estimators=200;, score=0.774 total time=   6.7s\n",
            "[CV 3/5; 17/27] START learning_rate=0.1, max_depth=15, n_estimators=200.........\n",
            "[CV 3/5; 17/27] END learning_rate=0.1, max_depth=15, n_estimators=200;, score=0.845 total time=   8.7s\n",
            "[CV 4/5; 17/27] START learning_rate=0.1, max_depth=15, n_estimators=200.........\n",
            "[CV 4/5; 17/27] END learning_rate=0.1, max_depth=15, n_estimators=200;, score=0.838 total time=   8.6s\n",
            "[CV 5/5; 17/27] START learning_rate=0.1, max_depth=15, n_estimators=200.........\n",
            "[CV 5/5; 17/27] END learning_rate=0.1, max_depth=15, n_estimators=200;, score=0.795 total time=   6.8s\n",
            "[CV 1/5; 18/27] START learning_rate=0.1, max_depth=15, n_estimators=300.........\n",
            "[CV 1/5; 18/27] END learning_rate=0.1, max_depth=15, n_estimators=300;, score=0.764 total time=  11.6s\n",
            "[CV 2/5; 18/27] START learning_rate=0.1, max_depth=15, n_estimators=300.........\n",
            "[CV 2/5; 18/27] END learning_rate=0.1, max_depth=15, n_estimators=300;, score=0.786 total time=  11.6s\n",
            "[CV 3/5; 18/27] START learning_rate=0.1, max_depth=15, n_estimators=300.........\n",
            "[CV 3/5; 18/27] END learning_rate=0.1, max_depth=15, n_estimators=300;, score=0.843 total time=  11.6s\n",
            "[CV 4/5; 18/27] START learning_rate=0.1, max_depth=15, n_estimators=300.........\n",
            "[CV 4/5; 18/27] END learning_rate=0.1, max_depth=15, n_estimators=300;, score=0.833 total time=  11.6s\n",
            "[CV 5/5; 18/27] START learning_rate=0.1, max_depth=15, n_estimators=300.........\n",
            "[CV 5/5; 18/27] END learning_rate=0.1, max_depth=15, n_estimators=300;, score=0.805 total time=  11.6s\n",
            "[CV 1/5; 19/27] START learning_rate=1, max_depth=5, n_estimators=100............\n",
            "[CV 1/5; 19/27] END learning_rate=1, max_depth=5, n_estimators=100;, score=0.779 total time=   1.1s\n",
            "[CV 2/5; 19/27] START learning_rate=1, max_depth=5, n_estimators=100............\n",
            "[CV 2/5; 19/27] END learning_rate=1, max_depth=5, n_estimators=100;, score=0.776 total time=   1.1s\n",
            "[CV 3/5; 19/27] START learning_rate=1, max_depth=5, n_estimators=100............\n",
            "[CV 3/5; 19/27] END learning_rate=1, max_depth=5, n_estimators=100;, score=0.819 total time=   1.2s\n",
            "[CV 4/5; 19/27] START learning_rate=1, max_depth=5, n_estimators=100............\n",
            "[CV 4/5; 19/27] END learning_rate=1, max_depth=5, n_estimators=100;, score=0.781 total time=   3.1s\n",
            "[CV 5/5; 19/27] START learning_rate=1, max_depth=5, n_estimators=100............\n",
            "[CV 5/5; 19/27] END learning_rate=1, max_depth=5, n_estimators=100;, score=0.802 total time=   1.2s\n",
            "[CV 1/5; 20/27] START learning_rate=1, max_depth=5, n_estimators=200............\n",
            "[CV 1/5; 20/27] END learning_rate=1, max_depth=5, n_estimators=200;, score=0.774 total time=   1.4s\n",
            "[CV 2/5; 20/27] START learning_rate=1, max_depth=5, n_estimators=200............\n",
            "[CV 2/5; 20/27] END learning_rate=1, max_depth=5, n_estimators=200;, score=0.781 total time=   1.4s\n",
            "[CV 3/5; 20/27] START learning_rate=1, max_depth=5, n_estimators=200............\n",
            "[CV 3/5; 20/27] END learning_rate=1, max_depth=5, n_estimators=200;, score=0.817 total time=   1.5s\n",
            "[CV 4/5; 20/27] START learning_rate=1, max_depth=5, n_estimators=200............\n",
            "[CV 4/5; 20/27] END learning_rate=1, max_depth=5, n_estimators=200;, score=0.786 total time=   1.6s\n",
            "[CV 5/5; 20/27] START learning_rate=1, max_depth=5, n_estimators=200............\n",
            "[CV 5/5; 20/27] END learning_rate=1, max_depth=5, n_estimators=200;, score=0.798 total time=   1.5s\n",
            "[CV 1/5; 21/27] START learning_rate=1, max_depth=5, n_estimators=300............\n",
            "[CV 1/5; 21/27] END learning_rate=1, max_depth=5, n_estimators=300;, score=0.769 total time=   3.6s\n",
            "[CV 2/5; 21/27] START learning_rate=1, max_depth=5, n_estimators=300............\n",
            "[CV 2/5; 21/27] END learning_rate=1, max_depth=5, n_estimators=300;, score=0.781 total time=   1.6s\n",
            "[CV 3/5; 21/27] START learning_rate=1, max_depth=5, n_estimators=300............\n",
            "[CV 3/5; 21/27] END learning_rate=1, max_depth=5, n_estimators=300;, score=0.821 total time=   1.9s\n",
            "[CV 4/5; 21/27] START learning_rate=1, max_depth=5, n_estimators=300............\n",
            "[CV 4/5; 21/27] END learning_rate=1, max_depth=5, n_estimators=300;, score=0.793 total time=   1.9s\n",
            "[CV 5/5; 21/27] START learning_rate=1, max_depth=5, n_estimators=300............\n",
            "[CV 5/5; 21/27] END learning_rate=1, max_depth=5, n_estimators=300;, score=0.800 total time=   1.9s\n",
            "[CV 1/5; 22/27] START learning_rate=1, max_depth=10, n_estimators=100...........\n",
            "[CV 1/5; 22/27] END learning_rate=1, max_depth=10, n_estimators=100;, score=0.779 total time=   1.4s\n",
            "[CV 2/5; 22/27] START learning_rate=1, max_depth=10, n_estimators=100...........\n",
            "[CV 2/5; 22/27] END learning_rate=1, max_depth=10, n_estimators=100;, score=0.779 total time=   3.3s\n",
            "[CV 3/5; 22/27] START learning_rate=1, max_depth=10, n_estimators=100...........\n",
            "[CV 3/5; 22/27] END learning_rate=1, max_depth=10, n_estimators=100;, score=0.821 total time=   1.5s\n",
            "[CV 4/5; 22/27] START learning_rate=1, max_depth=10, n_estimators=100...........\n",
            "[CV 4/5; 22/27] END learning_rate=1, max_depth=10, n_estimators=100;, score=0.786 total time=   1.4s\n",
            "[CV 5/5; 22/27] START learning_rate=1, max_depth=10, n_estimators=100...........\n",
            "[CV 5/5; 22/27] END learning_rate=1, max_depth=10, n_estimators=100;, score=0.771 total time=   1.4s\n",
            "[CV 1/5; 23/27] START learning_rate=1, max_depth=10, n_estimators=200...........\n",
            "[CV 1/5; 23/27] END learning_rate=1, max_depth=10, n_estimators=200;, score=0.767 total time=   1.7s\n",
            "[CV 2/5; 23/27] START learning_rate=1, max_depth=10, n_estimators=200...........\n",
            "[CV 2/5; 23/27] END learning_rate=1, max_depth=10, n_estimators=200;, score=0.779 total time=   1.6s\n",
            "[CV 3/5; 23/27] START learning_rate=1, max_depth=10, n_estimators=200...........\n",
            "[CV 3/5; 23/27] END learning_rate=1, max_depth=10, n_estimators=200;, score=0.824 total time=   1.8s\n",
            "[CV 4/5; 23/27] START learning_rate=1, max_depth=10, n_estimators=200...........\n",
            "[CV 4/5; 23/27] END learning_rate=1, max_depth=10, n_estimators=200;, score=0.788 total time=   3.7s\n",
            "[CV 5/5; 23/27] START learning_rate=1, max_depth=10, n_estimators=200...........\n",
            "[CV 5/5; 23/27] END learning_rate=1, max_depth=10, n_estimators=200;, score=0.776 total time=   1.8s\n",
            "[CV 1/5; 24/27] START learning_rate=1, max_depth=10, n_estimators=300...........\n",
            "[CV 1/5; 24/27] END learning_rate=1, max_depth=10, n_estimators=300;, score=0.767 total time=   1.9s\n",
            "[CV 2/5; 24/27] START learning_rate=1, max_depth=10, n_estimators=300...........\n",
            "[CV 2/5; 24/27] END learning_rate=1, max_depth=10, n_estimators=300;, score=0.776 total time=   1.8s\n",
            "[CV 3/5; 24/27] START learning_rate=1, max_depth=10, n_estimators=300...........\n",
            "[CV 3/5; 24/27] END learning_rate=1, max_depth=10, n_estimators=300;, score=0.821 total time=   2.2s\n",
            "[CV 4/5; 24/27] START learning_rate=1, max_depth=10, n_estimators=300...........\n",
            "[CV 4/5; 24/27] END learning_rate=1, max_depth=10, n_estimators=300;, score=0.788 total time=   4.0s\n",
            "[CV 5/5; 24/27] START learning_rate=1, max_depth=10, n_estimators=300...........\n",
            "[CV 5/5; 24/27] END learning_rate=1, max_depth=10, n_estimators=300;, score=0.769 total time=   2.1s\n",
            "[CV 1/5; 25/27] START learning_rate=1, max_depth=15, n_estimators=100...........\n",
            "[CV 1/5; 25/27] END learning_rate=1, max_depth=15, n_estimators=100;, score=0.767 total time=   1.4s\n",
            "[CV 2/5; 25/27] START learning_rate=1, max_depth=15, n_estimators=100...........\n",
            "[CV 2/5; 25/27] END learning_rate=1, max_depth=15, n_estimators=100;, score=0.783 total time=   1.4s\n",
            "[CV 3/5; 25/27] START learning_rate=1, max_depth=15, n_estimators=100...........\n",
            "[CV 3/5; 25/27] END learning_rate=1, max_depth=15, n_estimators=100;, score=0.810 total time=   1.5s\n",
            "[CV 4/5; 25/27] START learning_rate=1, max_depth=15, n_estimators=100...........\n",
            "[CV 4/5; 25/27] END learning_rate=1, max_depth=15, n_estimators=100;, score=0.793 total time=   1.5s\n",
            "[CV 5/5; 25/27] START learning_rate=1, max_depth=15, n_estimators=100...........\n",
            "[CV 5/5; 25/27] END learning_rate=1, max_depth=15, n_estimators=100;, score=0.774 total time=   3.4s\n",
            "[CV 1/5; 26/27] START learning_rate=1, max_depth=15, n_estimators=200...........\n",
            "[CV 1/5; 26/27] END learning_rate=1, max_depth=15, n_estimators=200;, score=0.762 total time=   1.6s\n",
            "[CV 2/5; 26/27] START learning_rate=1, max_depth=15, n_estimators=200...........\n",
            "[CV 2/5; 26/27] END learning_rate=1, max_depth=15, n_estimators=200;, score=0.790 total time=   1.6s\n",
            "[CV 3/5; 26/27] START learning_rate=1, max_depth=15, n_estimators=200...........\n",
            "[CV 3/5; 26/27] END learning_rate=1, max_depth=15, n_estimators=200;, score=0.817 total time=   1.9s\n",
            "[CV 4/5; 26/27] START learning_rate=1, max_depth=15, n_estimators=200...........\n",
            "[CV 4/5; 26/27] END learning_rate=1, max_depth=15, n_estimators=200;, score=0.793 total time=   1.8s\n",
            "[CV 5/5; 26/27] START learning_rate=1, max_depth=15, n_estimators=200...........\n",
            "[CV 5/5; 26/27] END learning_rate=1, max_depth=15, n_estimators=200;, score=0.774 total time=   1.8s\n",
            "[CV 1/5; 27/27] START learning_rate=1, max_depth=15, n_estimators=300...........\n",
            "[CV 1/5; 27/27] END learning_rate=1, max_depth=15, n_estimators=300;, score=0.774 total time=   3.8s\n",
            "[CV 2/5; 27/27] START learning_rate=1, max_depth=15, n_estimators=300...........\n",
            "[CV 2/5; 27/27] END learning_rate=1, max_depth=15, n_estimators=300;, score=0.786 total time=   1.9s\n",
            "[CV 3/5; 27/27] START learning_rate=1, max_depth=15, n_estimators=300...........\n",
            "[CV 3/5; 27/27] END learning_rate=1, max_depth=15, n_estimators=300;, score=0.821 total time=   2.2s\n",
            "[CV 4/5; 27/27] START learning_rate=1, max_depth=15, n_estimators=300...........\n",
            "[CV 4/5; 27/27] END learning_rate=1, max_depth=15, n_estimators=300;, score=0.798 total time=   2.2s\n",
            "[CV 5/5; 27/27] START learning_rate=1, max_depth=15, n_estimators=300...........\n",
            "[CV 5/5; 27/27] END learning_rate=1, max_depth=15, n_estimators=300;, score=0.774 total time=   2.2s\n",
            "Best parameters found:  {'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 300}\n",
            "Best score found:  0.8099999999999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "y_pred = grid.predict(valEmb)\n",
        "print('LGBM (with Grid Search): Val-accuracy: %.2f%%' % (100*accuracy_score(y_val, y_pred)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiO6ksp8D2Er",
        "outputId": "45a2023e-8306-45c7-f180-902a7e6a3e54"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LGBM (with Grid Search): Val-accuracy: 80.89%\n"
          ]
        }
      ]
    }
  ]
}